{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02343a19-8f8f-4a54-a5a1-f9cc0448c123",
   "metadata": {},
   "source": [
    "# DATA DOWNLOAD AND EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8ab28-7314-413a-ab54-78b5c4e77c5f",
   "metadata": {},
   "source": [
    "## 1. Project Context\n",
    "This projects aims to recognize fouls in soccer matches through multi-view videos. We use the SoccerNet-v2 dataset, wich provides detailed videos and annotations for training an validating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a8789-92a8-4c50-a678-c4a150a1a5c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This first notebook will cover the **installation** of the dataset to be used. This dataset is provided by _SoccerNet_ and contains over 3.000 multi-view videos for training, validation and testing, and provides 273 actions for the challenge.\n",
    "\n",
    "We start by installing the __SoccerNet-v2__, wich will provide us the necessary tools for the download of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b68c1-f588-4a37-b81f-b3af66fea530",
   "metadata": {},
   "source": [
    "## 2. Setup Instructions\n",
    "\n",
    "We use a virtual environment to manage the project's dependecies. This prevents conflicts with Python configuration between contributors.\n",
    "\n",
    "Before running this notebook, please ensure the following steps have been completed:\n",
    "\n",
    "1. **Activate the Virtual Environment**:\n",
    "   Make sure you have activated the virtual environment where all the dependencies are installed. If you haven't created a virtual environment yet, you can do so with the following commands:\n",
    "\n",
    "   ```sh\n",
    "   python -m venv venv\n",
    "   source venv\\Scripts\\activate  # On Windows\n",
    "\n",
    "2. **Make sure you have all the dependencies installed:**\n",
    "   You can do so by running the following command:\n",
    "\n",
    "   ```sh\n",
    "   pip install -r requirements.txt\n",
    "   \n",
    "\n",
    "4. **Run Jupyter Lab once the Virtual Environment is running**:\n",
    "   Run the following command:\n",
    "\n",
    "   ```sh\n",
    "   jupyter lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e87e29-c6a4-421e-9af8-5e542ecdf66b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install SoccerNet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490ba6a-76c0-4f2c-8605-ab380db57ca8",
   "metadata": {},
   "source": [
    "## 3. Environment Variables\n",
    "\n",
    "For this download we need to adquire a **license of use** through this [link](https://docs.google.com/forms/d/e/1FAIpQLSfYFqjZNm4IgwGnyJXDPk2Ko_lZcbVtYX73w5lf6din5nxfmA/viewform). To avoid hardcoding sensitive information in the code, we use environment variables wich are crucial for keeping sensitive information. Make sure to properly follow the next steps. \n",
    "\n",
    "#### Steps to Set Up\n",
    "\n",
    "1. Copy the `env_example` file and rename it as `.env` in the root directory of this project.\n",
    "3. Replace `your_password_here` with your actual Soccernet password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffd5938-f5ed-48ec-8f09-3abd98d174c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SoccerNet\n",
    "from SoccerNet.Downloader import SoccerNetDownloader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "mySoccerNetDownloader = SoccerNetDownloader(LocalDirectory=\"../data\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "password = os.getenv('SOCCERNET_PASSWORD')\n",
    "mySoccerNetDownloader.password = password"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1bfb2b-4a1f-45d1-b197-f35a1a1f5ccb",
   "metadata": {},
   "source": [
    "## 4. Dataset description and download\n",
    "\n",
    "- train: Standard training data for models, with complete annotations.\n",
    "- valid: Validation data to evaluate performance during development.\n",
    "- test: Test data with non-public labels (typically used for competitions).\n",
    "- challenge: A subset of data specific to official competition or specific challenges, such as Multi-View Foul Recognition.\n",
    "\n",
    "We need the dataset related withe *Multi-view Foul Recognition* challenge, known as __mvfouls__. Also we will download only videos in 720p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbb921-bf3d-4d47-9370-3361786546c4",
   "metadata": {},
   "source": [
    "**IMPORTANT: by executing the following code the installation will start, it can last a long time, so be sure of executing it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d40e07-68af-4834-b05e-29e609a66661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ../data\\mvfouls\\train_720p.zip...: : 13.3GiB [14:30, 15.2MiB/s]                                            \n",
      "Downloading ../data\\mvfouls\\valid_720p.zip...: : 1.85GiB [02:05, 14.7MiB/s]                                            \n",
      "Downloading ../data\\mvfouls\\test_720p.zip...: : 1.45GiB [01:42, 14.1MiB/s]                                             \n",
      "Downloading ../data\\mvfouls\\challenge_720p.zip...: : 1.37GiB [01:31, 14.9MiB/s]                                        \n"
     ]
    }
   ],
   "source": [
    "mySoccerNetDownloader.downloadDataTask(task=\"mvfouls\", split=[\"train\",\"valid\",\"test\",\"challenge\"], password = psswrd, version = \"720p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bda05b-7698-4b3f-a28e-52f912867072",
   "metadata": {},
   "source": [
    "The dataset download is in _.zip_ files, which must be **unzipped**.\n",
    "\n",
    "The dataset consists of **3,901 actions**. Each action is composed of at least two videos showing the live action and at least one replay.\n",
    "\n",
    "The dataset is divided into:\n",
    "- Training set (2916 actions).\n",
    "- Validation set (411 actions).\n",
    "- Test set (301 actions).\n",
    "- Challenge set (273 actions without the annotations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e9fad-9aa3-48eb-9d20-13fd798bd08d",
   "metadata": {},
   "source": [
    "## 4. Download Verification\n",
    "\n",
    "It is important to verify that the videos and labels have been downloaded correctly.\n",
    "\n",
    "We will then use _OpenCV_ to check that the videos and tags have been downloaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6ddedc-cf39-4492-965b-94f46af81ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "train_dataset_path = \"../data/mvfouls/train_720p\"\n",
    "\n",
    "training_videos_path = os.path.join(train_dataset_path)\n",
    "example_video = os.path.join(training_videos_path, \"action_1/clip_0.mp4\")\n",
    "\n",
    "def visualize_video(video_path, num_frames=100):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Video couldn't be opened: {video_path}\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened() and frame_count < num_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of the video or error while reading the frame.\")\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Video Frame', frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "visualize_video(example_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fbd09-ae6d-4432-9195-b5b378a3927e",
   "metadata": {},
   "source": [
    "We will also verify that the tags are in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee329b86-8b66-4199-b85a-e730fb6fa506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fo etiqueted actions: 2916\n",
      "Action index: 0\n",
      "Action details: {'UrlLocal': 'england_epl\\\\2014-2015\\\\2015-02-21 - 18-00 Chelsea 1 - 1 Burnley', 'Offence': 'Offence', 'Contact': 'With contact', 'Bodypart': 'Upper body', 'Upper body part': 'Use of shoulder', 'Action class': 'Challenge', 'Severity': '1.0', 'Multiple fouls': '', 'Try to play': '', 'Touch ball': '', 'Handball': 'No handball', 'Handball offence': '', 'Clips': [{'Url': 'Dataset/Train/action_0/clip_0', 'Camera type': 'Main camera center', 'Timestamp': 1730826, 'Replay speed': 1.0}, {'Url': 'Dataset/Train/action_0/clip_1', 'Camera type': 'Close-up player or field referee', 'Timestamp': 1744173, 'Replay speed': 1.8}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "labels_path = os.path.join(train_dataset_path, \"annotations.json\")\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total fo etiqueted actions: {data['Number of actions']}\")\n",
    "for action_index, action_data in data['Actions'].items():\n",
    "    print(f\"Action index: {action_index}\")\n",
    "    print(f\"Action details: {action_data}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f41700-1247-4324-ad2a-5b2ff4dd0bda",
   "metadata": {},
   "source": [
    "## 5. Preliminary Data Analysis\n",
    "\n",
    "In this brief section we perform a preliminary analysis of the downloaded data by visualizing some action's tags\n",
    "We will extract the information of three actions and also visualize and example of two different clips from the same action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c21787d-8951-4801-9bdc-60e0f2d4c3c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action number 0:\n",
      "  > Offence: Offence\n",
      "  > Bodypart: Upper body\n",
      "  > Action type: Challenge\n",
      "  > Severity: 1.0\n",
      "    - Clip - Main camera center: Dataset/Train/action_0/clip_0 @ 1730826ms\n",
      "    - Clip - Close-up player or field referee: Dataset/Train/action_0/clip_1 @ 1744173ms\n",
      "------------------------------------------------------\n",
      "Action number 1:\n",
      "  > Offence: Offence\n",
      "  > Bodypart: Under body\n",
      "  > Action type: Tackling\n",
      "  > Severity: 3.0\n",
      "    - Clip - Main camera center: Dataset/Train/action_1/clip_0 @ 2400217ms\n",
      "    - Clip - Close-up player or field referee: Dataset/Train/action_1/clip_1 @ 2415695ms\n",
      "------------------------------------------------------\n",
      "Action number 2:\n",
      "  > Offence: Offence\n",
      "  > Bodypart: Under body\n",
      "  > Action type: Standing tackling\n",
      "  > Severity: 3.0\n",
      "    - Clip - Main camera center: Dataset/Train/action_2/clip_0 @ 206821ms\n",
      "    - Clip - Close-up player or field referee: Dataset/Train/action_2/clip_1 @ 230429ms\n",
      "    - Clip - Close-up player or field referee: Dataset/Train/action_2/clip_2 @ 238257ms\n"
     ]
    }
   ],
   "source": [
    "actions = data['Actions']\n",
    "\n",
    "count = 3\n",
    "\n",
    "for action_id, action in actions.items():\n",
    "    print(f\"Action number {action_id}:\")\n",
    "    print(f\"  > Offence: {action['Offence']}\")\n",
    "    print(f\"  > Bodypart: {action['Bodypart']}\")\n",
    "    print(f\"  > Action type: {action['Action class']}\")\n",
    "    print(f\"  > Severity: {action['Severity']}\")\n",
    "    \n",
    "    for clip in action['Clips']:\n",
    "        clip_url = clip['Url']\n",
    "        camera_type = clip['Camera type']\n",
    "        timestamp = clip['Timestamp']\n",
    "        print(f\"    - Clip - {camera_type}: {clip_url} @ {timestamp}ms\")\n",
    "    count -= 1\n",
    "    if count == 0:\n",
    "        break\n",
    "    print(f\"------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62aa32-4d29-49c8-bd7f-1392a6e81ecc",
   "metadata": {},
   "source": [
    "In this fragment we will show both clips of an action currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4434a2dc-836a-4f8a-888e-d2bc206fa5c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "train_dataset_path = \"../data/mvfouls/train_720p\"\n",
    "\n",
    "clip_0_path = os.path.join(train_dataset_path, \"action_1/clip_0.mp4\")\n",
    "clip_1_path = os.path.join(train_dataset_path, \"action_1/clip_1.mp4\")\n",
    "\n",
    "\n",
    "def visualize_clips_synchronized(clip_1_path, clip_2_path, timestamp_1=0, timestamp_2=0, num_frames=100):\n",
    "    cap1 = cv2.VideoCapture(clip_1_path)\n",
    "    cap2 = cv2.VideoCapture(clip_2_path)\n",
    "    \n",
    "    if not cap1.isOpened() or not cap2.isOpened():\n",
    "        print(f\"Error opening videos: {clip_1_path}, {clip_2_path}\")\n",
    "        return\n",
    "\n",
    "    fps1 = cap1.get(cv2.CAP_PROP_FPS)\n",
    "    fps2 = cap2.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    start_frame_1 = int(timestamp_1 / 1000 * fps1)\n",
    "    start_frame_2 = int(timestamp_2 / 1000 * fps2)\n",
    "\n",
    "    cap1.set(cv2.CAP_PROP_POS_FRAMES, start_frame_1)\n",
    "    cap2.set(cv2.CAP_PROP_POS_FRAMES, start_frame_2)\n",
    "\n",
    "    frame_count = 0\n",
    "    while frame_count < num_frames:\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            print(\"End of one of the videos or error when reading the frame.\")\n",
    "            break\n",
    "\n",
    "        combined_frame = cv2.hconcat([frame1, frame2])\n",
    "        cv2.imshow('Synchronized Clips', combined_frame)\n",
    "\n",
    "        if cv2.waitKey(int(1000 / max(fps1, fps2))) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "visualize_clips_synchronized(clip_0_path, clip_1_path, timestamp_1=0, timestamp_2=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bbdd3-b9e8-45ad-bf92-1d79f04b8fd5",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "In the next notebook, we will perform more comprehensive data exploration to understand the distribution and characteristics of the dataset and tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
